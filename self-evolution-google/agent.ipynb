{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13865df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_tavily import TavilySearch\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "230fcc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(search_query: str, web_results: str, environment_feedback: str, model: ChatGoogleGenerativeAI) -> str:\n",
    "    print(\"Generating answer...\")\n",
    "    prompt = (\n",
    "        \"\"\"\n",
    "        Given the search query, web results and environment feedback, provide a concise and accurate answer.\n",
    "        \n",
    "        1. Search Query: {search_query}\n",
    "        2. Web Results: {web_results}\n",
    "        3. Environment Feedback: {environment_feedback}\n",
    "\n",
    "        Focus on Facts: Prioritize numbers, dates, definitions, and distinct arguments.\n",
    "        If environment feedback is provided, incorporate it into the answer.\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    return model.invoke(\n",
    "        prompt.format(\n",
    "            search_query=search_query, \n",
    "            web_results=web_results, \n",
    "            environment_feedback=environment_feedback\n",
    "        )\n",
    "    ).content\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5327626c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def web_search(query: str) -> str:\n",
    "    \"\"\"Perform a web search and return the results\"\"\"\n",
    "    print(\"Performing Web Search...\")\n",
    "\n",
    "    web_search_tool = TavilySearch(max_results=20)\n",
    "    web_results = web_search_tool.invoke({\"query\": query})\n",
    "\n",
    "    return _parse_web_search_results(web_results)\n",
    "\n",
    "\n",
    "def _parse_web_search_results(web_search_result) -> str:\n",
    "    \"\"\"Parse the web search results and filter based on relevance score\"\"\"\n",
    "\n",
    "    if not web_search_result or \"results\" not in web_search_result:\n",
    "        return \"\"\n",
    "\n",
    "    search_answers = []\n",
    "    for result in web_search_result[\"results\"]:\n",
    "        search_content = result[\"content\"]\n",
    "        search_answers.append(search_content)\n",
    "\n",
    "    return '\\n'.join(search_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955a980a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_environment_feedback(answer: str, search_query: str, web_results: str) -> str:\n",
    "    print(\"Generating Environment Feedback...\")\n",
    "    prompt = (\n",
    "        \"\"\"\n",
    "        Given the answer, search query, and web results, provide feedback on the answer provided for the search query.\n",
    "        Keep the feedback concise and focused on factual accuracy.\n",
    "        Take the Web Results as a source of truth when evaluating the answer.\n",
    "        \n",
    "        1. Answer: {answer}\n",
    "        2. Search Query: {search_query}\n",
    "        3. Web Results: {web_results}\n",
    "        \"\"\"\n",
    "    )\n",
    "    model = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\",)\n",
    "\n",
    "    return model.invoke(\n",
    "        prompt.format(\n",
    "            answer=answer,\n",
    "            search_query=search_query,\n",
    "            web_results=web_results\n",
    "        )\n",
    "    ).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bb9486",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_REFINEMENTS = 3\n",
    "\n",
    "def start_self_evolution(model: ChatGoogleGenerativeAI, search_query: str):\n",
    "    refinement_count = 0\n",
    "    answer = \"\"\n",
    "    environment_feedback = \"\"\n",
    "    web_results = web_search(search_query)\n",
    "\n",
    "    while refinement_count < MAX_REFINEMENTS:\n",
    "        print(f\"Refinement Iteration: {refinement_count + 1}\")\n",
    "        answer = generate_answer(search_query, web_results, environment_feedback, model)\n",
    "        environment_feedback = get_environment_feedback(answer, search_query, web_results)\n",
    "        refinement_count += 1\n",
    "    \n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58b1de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def candidates_crossover(evolved_answers: list[str], search_query: str) -> str:\n",
    "    \"\"\"Combine multiple evolved answers into a single answer\"\"\"\n",
    "    print(\"Performing Crossover of Candidates...\")\n",
    "    model = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\")\n",
    "    prompt = (\n",
    "        \"\"\"\n",
    "        Your task is to research a topic and try to fulfill the user query in the <user> tags.\n",
    "        <instructions>\n",
    "        You are given a list of candidate answers in <answer_list> tags below. Combine them into a single answer so that,\n",
    "        + it best fulfills the initial user query in the <user> tags.\n",
    "        + If there are conflicting information, try to reconcile them in a logically sound way.\n",
    "        </instructions>\n",
    "        Here is the user query.\n",
    "        <user>\n",
    "        {query}\n",
    "        </user>\n",
    "        Here is the list of candidate answers you need to merge.\n",
    "        <answer_list>\n",
    "        {answer_list}\n",
    "        </answer_list>\n",
    "        Only output a combined answer from the answers in <answer_list>. Do NOT use other information.\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    return model.invoke(\n",
    "        prompt.format(\n",
    "            query=search_query,\n",
    "            answer_list='\\n'.join(\n",
    "                [ f\"Candidate Answer: {evolved_answer}\" for evolved_answer in evolved_answers]\n",
    "            )\n",
    "        )\n",
    "    ).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a02af41",
   "metadata": {},
   "outputs": [],
   "source": [
    "CANDIDATES = 3\n",
    "CANDIDATES_CONFIGURATION = [\n",
    "    {\n",
    "        \"top_k\": 30,\n",
    "        \"temperature\": 0.5,\n",
    "    },\n",
    "    {\n",
    "        \"top_k\": 40,\n",
    "        \"temperature\": 1.0,\n",
    "    },\n",
    "    {\n",
    "        \"top_k\": 50,\n",
    "        \"temperature\": 1.5,\n",
    "    },\n",
    "]\n",
    "\n",
    "def perform_self_evolution(search_query: str):\n",
    "    evolved_answers = []\n",
    "    for index, candidate_config in enumerate(CANDIDATES_CONFIGURATION):\n",
    "        print(f\"\\n Spawning Candidate: {index + 1} \\n\")\n",
    "        model = ChatGoogleGenerativeAI(\n",
    "            model=\"gemini-2.5-flash\",\n",
    "            temperature=candidate_config[\"temperature\"],\n",
    "            top_k=candidate_config[\"top_k\"],\n",
    "        )\n",
    "        evolved_answer = start_self_evolution(model, search_query)\n",
    "        evolved_answers.append(evolved_answer)\n",
    "    \n",
    "    return candidates_crossover(evolved_answers, search_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3174942a",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "query = \"What are the investment philosophies of Duan Yongping, Warren Buffett, and Charlie Munger?\"\n",
    "crossover_evolved_answer = perform_self_evolution(query)\n",
    "print(\"\\n Final Evolved Answer after Crossover: \\n\")\n",
    "print(crossover_evolved_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c43a3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "\n",
    "def perform_self_evolution_in_parallel(search_query: str):\n",
    "    future_responses = []\n",
    "    evolved_answers = []\n",
    "    with ThreadPoolExecutor(max_workers=3) as executor:\n",
    "        for index, candidate_config in enumerate(CANDIDATES_CONFIGURATION):\n",
    "            print(f\"\\n Spawning Candidate: {index + 1} \\n\")\n",
    "            model = ChatGoogleGenerativeAI(\n",
    "                model=\"gemini-2.5-flash\",\n",
    "                temperature=candidate_config[\"temperature\"],\n",
    "                top_k=candidate_config[\"top_k\"],\n",
    "            )\n",
    "            future_response = executor.submit(start_self_evolution, model, search_query)\n",
    "            future_responses.append(future_response)\n",
    "\n",
    "    evolved_answers = [future.result() for future in future_responses]\n",
    "    return candidates_crossover(evolved_answers, search_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fb07c572",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_without_self_evolution(search_query: str) -> str:\n",
    "    web_results = web_search(search_query)\n",
    "    model = ChatGoogleGenerativeAI(model=\"gemini-2.5-pro\")\n",
    "    answer = generate_answer(search_query, web_results, \"\", model)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8c2f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_answer = search_without_self_evolution(query)\n",
    "print(\"\\n Search Answer without Self Evolution: \\n\")\n",
    "print(search_answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saurav-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
