{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from bs4 import BeautifulSoup as Soup\n",
    "from langchain_community.document_loaders.recursive_url_loader import RecursiveUrlLoader\n",
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint, HuggingFacePipeline\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain import hub\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNOWLEDGE_BASE_URLS = [\n",
    "    'https://www.google.com/about/careers/applications/jobs/results?q=%22software%20engineer%22&employment_type=FULL_TIME&company=Google&location=India'\n",
    "]\n",
    "\n",
    "BASE_URL = 'https://www.google.com/about/careers/applications/jobs/results?q=%22software%20engineer%22&employment_type=FULL_TIME&company=Google&location=India'\n",
    "\n",
    "GOOGLE_JOB_LINK_PREFIX = 'https://www.google.com/about/careers/applications/'\n",
    "FETCHED_GOOGLE_JOB_URL_PREFIX = 'jobs/results/'\n",
    "\n",
    "MINIMUM_QUALIFICATION_QUESTION = 'What are the Minimum Qualifications in the job?'\n",
    "PREFERRED_QUALIFICATION_QUESTION = 'What are the Preferred Qualifications in the job?'\n",
    "JOB_RESPONSIBILITIES_QUESTION = 'What are the Responsibilities in the job?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = RecursiveUrlLoader(\n",
    "    url=BASE_URL, max_depth=2, extractor=lambda x: Soup(x, \"html.parser\").text\n",
    ")\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_urls(base_url):\n",
    "    try:\n",
    "        # Fetch the HTML content from the URL\n",
    "        response = requests.get(base_url)\n",
    "        response.raise_for_status()  # This will raise an exception for bad responses (4xx or 5xx)\n",
    "\n",
    "        # Parse the HTML content\n",
    "        soup = Soup(response.text, 'html.parser')\n",
    "\n",
    "        # Find all 'a' tags (anchor tags) which contain links\n",
    "        links = soup.find_all('a')\n",
    "\n",
    "        # Store the URLs in a list\n",
    "        all_urls = []\n",
    "        for link in links:\n",
    "            href = link.get('href')  # Get the value of the 'href' attribute\n",
    "            if href:\n",
    "                all_urls.append(href)\n",
    "\n",
    "        return all_urls\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def get_google_job_urls(urls):\n",
    "    google_job_urls = []\n",
    "    \n",
    "    for url in urls:\n",
    "        if url.startswith(FETCHED_GOOGLE_JOB_URL_PREFIX):\n",
    "            google_job_urls.append(GOOGLE_JOB_LINK_PREFIX + url)\n",
    "    \n",
    "    return google_job_urls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url_content(url):\n",
    "    return WebBaseLoader(url).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = get_all_urls(BASE_URL)\n",
    "print(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_job_urls = get_google_job_urls(urls)\n",
    "print(google_job_urls)\n",
    "print(len(google_job_urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = get_url_content(google_job_urls[0])\n",
    "print(content[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(load_from_hugging_face=False):\n",
    "    if load_from_hugging_face:\n",
    "        llm = HuggingFaceEndpoint(\n",
    "            repo_id=\"openai/gpt-oss-120b\",\n",
    "            task=\"text-generation\",\n",
    "            provider=\"auto\",  # set your provider here\n",
    "        )\n",
    "\n",
    "        return ChatHuggingFace(llm=llm)\n",
    "    \n",
    "    return ChatOpenAI(model=\"gpt-4\", temperature=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_resume_content():\n",
    "    profile = open('./knowledge_base/resume.md', 'r')\n",
    "    profile_content = profile.read()\n",
    "    return profile_content\n",
    "\n",
    "\n",
    "def get_minimum_qualifications(job_profile_content):\n",
    "    prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "    llm = get_model(load_from_hugging_face=True)\n",
    "    rag_chain = prompt | llm | StrOutputParser()\n",
    "    \n",
    "    response = rag_chain.invoke(\n",
    "        {\"context\": job_profile_content, \"question\": MINIMUM_QUALIFICATION_QUESTION})\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "def get_preferred_qualification(job_profile_content):\n",
    "    prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "    llm = get_model(load_from_hugging_face=True)\n",
    "    rag_chain = prompt | llm | StrOutputParser()\n",
    "    \n",
    "    response = rag_chain.invoke(\n",
    "        {\"context\": job_profile_content, \"question\": PREFERRED_QUALIFICATION_QUESTION})\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "def get_job_responsibilities(job_profile_content):\n",
    "    prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "    llm = get_model(load_from_hugging_face=True)\n",
    "    rag_chain = prompt | llm | StrOutputParser()\n",
    "    \n",
    "    response = rag_chain.invoke(\n",
    "        {\"context\": job_profile_content, \"question\": JOB_RESPONSIBILITIES_QUESTION})\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "def analyze_resume_against_minimum_qualifications(resume_content, minimum_qual):\n",
    "    model = get_model(load_from_hugging_face=True)\n",
    "\n",
    "    grade_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", MINIMUM_QUALIFICATION_PROMPT),\n",
    "            (\"human\", \"Minimum Qualification: \\n\\n {min_qual} \\n\\n Candidate's resume: {resume}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    grade_chain = grade_prompt | model | StrOutputParser()\n",
    "    response = grade_chain.invoke({\"min_qual\": minimum_qual, \"resume\": resume_content})\n",
    "\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_content = get_resume_content()\n",
    "response = analyze_resume_against_minimum_qualifications(resume_content, min_quals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader('knowledge_base/resume.pdf')\n",
    "pages = []\n",
    "async for page in loader.alazy_load():\n",
    "    pages.append(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saurav Prateek\n",
      "Gurugram, India +91 9911453471 srvptk97@gmail.com Linkedin | Github  \n",
      "Professional Experience (5+ years)\n",
      " Google\n",
      "Nov 2022 - April 2025Web Solutions Engineer - II (L4) | gTech (Google Technical Services)\n",
      "Designed and developed the Ads Headroom Planning and Projection tool for Video based campaigns in Google Ads from \n",
      "scratch allowing Ads Specialists to project and analyse future performances of 500+ Video Campaigns and 100+ AdWords \n",
      "Accounts at once.\n",
      "Implemented an asynchronous approach to import multiple AdWords Account in-parallel using PromiseGraphs (an async \n",
      "framework) and Fire-and-Forget pattern. This helped in successfully importing 15+ dimensions of Campaigns having across \n",
      "1 Million+ results within 30 seconds.\n",
      "Designed and implemented SQL data pipeline from scratch for crunching Google Ads’ LCS and SMB pods data. The pipeline \n",
      "had 30+ nodes forming a graphical structure and processing 1.5 Million+ quarterly ads sales data.\n",
      "Awarded a Spot Bonus from the Director of Google’s Tools Automation and Infrastructure (TAI) team for delivering the Ads \n",
      "Headroom Planning product under a strict deadline from scratch.\n",
      "• \n",
      "•\n",
      "•\n",
      "•  \n",
      "Jul 2021 - Nov 2022Web Solutions Engineer - I (L3) | gTech (Google Technical Services)\n",
      "Led the tool enabling Bulk Validation/Implementation support for 10+ Google Ads Entities involving - Site-links, Callouts, \n",
      "Campaigns, Ad Groups, Product Groups, Listing Groups and much more. This led to time savings of 60 minutes per case.\n",
      "Implemented AdWords streaming API to support the import of 100K+ response rows per API request.\n",
      "Implemented Caching for the Product Groups of the AdWord accounts to reduce the API response time by 82%.\n",
      "Developed a recursion and backtracking based solution to support the real-time import of hierarchical Product Groups (a \n",
      "google ads-entity) in bulk.\n",
      "• \n",
      "•\n",
      "•\n",
      "•\n",
      "  \n",
      " GeeksForGeeks Noida, India\n",
      "Jun 2019 - Jul 2021Software Development Engineer - 1\n",
      "Designed a Microservice for handling incoming requests on the platform. Reduced load over 53% on the Database \n",
      "through caching.\n",
      "Worked on Amazon DynamoDB, designing scalable and eﬃcient Database schema architectures. and product’s web \n",
      "application including backend (Core PHP, AJAX, Dynamo DB, SQL) and front-end (HTML5, CSS3, JS, Bootstrap 3).\n",
      "• \n",
      "•  \n",
      "Projects & Open-Source contributions\n",
      "LangChain - Google Vertex AI View Repo\n",
      "My ContributionsOpen Source Contributor on Github\n",
      "Involved as an active open source contributor for the release of version 2.0.14.\n",
      "Added the support for maximum tokens in the Vertex AI Chat LLM.\n",
      "Improved the performance of the Image Bytes Loader module by caching the instance of the module and avoid creating \n",
      "repeated instance of GCS (Google Cloud Storage) client on every call. \n",
      "• \n",
      "•\n",
      "•\n",
      "Education\n",
      " KCC Institute of Technology and Management (AKTU University) Greater Noida, India\n",
      "Aug 2015 - Jun 2019B.Tech | Computer Science and Engineering (CS)\n",
      "Completed the entire course work with an aggregate of 82% and passed with Distinction + Honours• \n",
      "Gurugram, India\n",
      "May 2025 - PresentSenior Web Solutions Engineer  (L5) | gTech (Google Technical Services)\n",
      "Leading Headroom tool that enables Video Specialists across LCS and GCS to perform video headroom planning for their \n",
      "book of business using historical data from their advertiser, a variety of methodologies to project headroom forecast, and \n",
      "multiple planning tools as a part of the process.\n",
      "•\n",
      "Micrograd - A Neural Network from scratch View Repo\n",
      "Techstack: Java, Python\n",
      "Designed a Value class to express the mathematical equations in the form of Graph and perform backtracking for \n",
      "backpropagation involving gradient calculation and parameter (weights + bias) updates.\n",
      "Designed a Neuron with collection of weights and a bias along with Layer (collection of Neurons) and Multi-Layer \n",
      "Perceptron (collection of Layers) to build the entire Neural network architecture from scratch.\n",
      "• \n",
      "•\n",
      "Systems That Scale - My engineering Newsletter View Newsletter\n",
      "Newsletter has 30K+ Subscribers\n",
      "My software Engineering newsletter on Linkedin is subscribed by over 30,000+ Engineers and Leaders.\n",
      "It has 50+ editions on diverse Software Engineering topics including Distributed Systems, System Design, Low Level Design, \n",
      "Machine Learning, LLM and much more.\n",
      "• \n",
      "•\n",
      "Achievements & Certiﬁcates\n",
      "Recognised Content Creator in the developer community\n",
      "60K+ Followers on Linkedin and 2K+ Subscribers on Youtube\n",
      "I actively create Software Engineering and related content on Linkedin and YouTube. My Youtube channel is subscribed by \n",
      "2000+ engineers and my Linkedin proﬁle has a follower base of 60,000+ Engineers and Leaders.\n",
      "I have delivered multiple in-person tech-talks in numerous events organized by Google Developer Groups (GDG) and \n",
      "others. My Commudle proﬁle holds my list of successfully delivered technical talks.\n",
      "• \n",
      "•\n",
      "• \n",
      "•\n",
      "•\n",
      "Professional / Organizational achievements\n",
      "Developed an AI powered workﬂow that won a second place in the org wide global Hackathon - Hack-a-gTech. [View Award]\n",
      "Promoted from L3 to L4 position within a span of 1.5 years and from L4 to L5 in the span of 2.5 years.\n",
      "Awarded a Spot bonus by the Vice President of gTech at Google for Coding Excellence [View].\n",
      "• \n",
      "•\n",
      "•\n",
      "Competitive Programming achievements\n",
      "Competed in ACM ICPC Regionals and secured a team rank of 73 all over India.\n",
      "4 Star rated programmer on Codechef and a Specialist on CodeForces. Holds a Contest rating of 1851 on LeetCode.\n",
      "Successfully cleared Codechef Data Structure and Algorithms Program (CCDSAP) both Foundation and Advanced level.\n",
      "Internship Experience\n",
      "Birla Institute of Technology (BIT Mesra)\n",
      "Patna, India\n",
      "Jan 2019 - Feb 2019Research Project Intern\n",
      "Developed a Generalised Model in order to increase the Accuracy and Attack Detection Ratio (ADR) of the System through \n",
      "Decision Tree Model carried on the UNSW_NB_15 Intrusion Dataset.\n",
      "India Meteorological Department (IMD)\n",
      "New Delhi, India\n",
      "May 2018 - Jul 2018Project Intern\n",
      "Developed a Web Application that can generate an Intensity Heat Map of Earthquake at speciﬁc Latitude, Longitude position. \n",
      "Used Google Maps API to generate a Heat Map.\n",
      "• \n",
      "• \n",
      "LangPost - An AI Agent that creates a Linkedin post for you! View Repo\n",
      "Techstack: TypeScript, LangGraph (framework by LangChain)  \n",
      "An AI agent that used Few-Shot prompting to understand the patterns and curates a LinkedIn post for a given context.\n",
      "It learns the behaviour and patterns from the labelled datasets to curate the content in the author’s style of writing.\n",
      "The project was appreciated and shared by LangChain on their Linkedin page. [View]\n",
      "• \n",
      "•\n",
      "•\n",
      "Deep Learning Specialization\n",
      "Completed “Neural Networks and Deep Learning” course as a part of the specialization. View Certiﬁcate\n",
      "Completed “Improving Deep Neural Networks: Hyperparameter Tuning, Regularization and Optimization” course as a part \n",
      "of the specialization. View Certiﬁcate\n",
      "Completed “Sequence Models (NLP)” as a part of the specialization. View Certiﬁcate\n",
      "• \n",
      "•\n",
      "•\n"
     ]
    }
   ],
   "source": [
    "for page in pages:\n",
    "    print(page.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sauravprateek/Documents/saurav-codes/AI-Engineering-101/saurav-env/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Using MXFP4 quantized models requires a GPU",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      4\u001b[39m local_model_path = \u001b[33m\"\u001b[39m\u001b[33m../gpt-oss-20b\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Load the tokenizer\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# tokenizer = AutoTokenizer.from_pretrained(local_model_path)\u001b[39;00m\n\u001b[32m      8\u001b[39m \n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Load the model, specifying the local path\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m model = \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocal_model_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/saurav-codes/AI-Engineering-101/saurav-env/lib/python3.13/site-packages/transformers/models/auto/auto_factory.py:600\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    598\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_class.config_class == config.sub_configs.get(\u001b[33m\"\u001b[39m\u001b[33mtext_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    599\u001b[39m         config = config.get_text_config()\n\u001b[32m--> \u001b[39m\u001b[32m600\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    601\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    602\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    603\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    604\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    605\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    606\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/saurav-codes/AI-Engineering-101/saurav-env/lib/python3.13/site-packages/transformers/modeling_utils.py:316\u001b[39m, in \u001b[36mrestore_default_torch_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    314\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    318\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/saurav-codes/AI-Engineering-101/saurav-env/lib/python3.13/site-packages/transformers/modeling_utils.py:4879\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4876\u001b[39m     hf_quantizer = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4878\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4879\u001b[39m     \u001b[43mhf_quantizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_environment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4880\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4881\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4882\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4883\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4884\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4885\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4886\u001b[39m     torch_dtype = hf_quantizer.update_torch_dtype(torch_dtype)\n\u001b[32m   4887\u001b[39m     device_map = hf_quantizer.update_device_map(device_map)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/saurav-codes/AI-Engineering-101/saurav-env/lib/python3.13/site-packages/transformers/quantizers/quantizer_mxfp4.py:60\u001b[39m, in \u001b[36mMxfp4HfQuantizer.validate_environment\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     56\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUsing mxfp4 quantization requires torch\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     57\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease install the latest version of torch ( pip install --upgrade torch )\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     58\u001b[39m     )\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch.cuda.is_available():\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mUsing MXFP4 quantized models requires a GPU\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_accelerate_available():\n\u001b[32m     63\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mUsing mxfp4 requires Accelerate: `pip install accelerate`\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mRuntimeError\u001b[39m: Using MXFP4 quantized models requires a GPU"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Specify the local directory where your model files are stored\n",
    "local_model_path = \"../gpt-oss-20b\"\n",
    "\n",
    "# Load the tokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(local_model_path)\n",
    "\n",
    "# Load the model, specifying the local path\n",
    "model = AutoModelForCausalLM.from_pretrained(local_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The capital of France is **Paris**. It is not only the political and administrative center of the country but also a major cultural, historical, and economic hub. Paris is famous for landmarks like the Eiffel Tower, the Louvre Museum, and Notre-Dame Cathedral.', additional_kwargs={}, response_metadata={'token_usage': {'prompt_tokens': 10, 'total_tokens': 64, 'completion_tokens': 54}, 'model_name': 'mistral-large-latest', 'model': 'mistral-large-latest', 'finish_reason': 'stop'}, id='run--c8a4f63f-e207-4a6a-b076-0b4f73d4af0a-0', usage_metadata={'input_tokens': 10, 'output_tokens': 54, 'total_tokens': 64})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_mistralai import ChatMistralAI\n",
    "\n",
    "llm = ChatMistralAI(\n",
    "    model=\"mistral-large-latest\",\n",
    "    temperature=0,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")\n",
    "\n",
    "llm.invoke(\"What is the capital of France?\")  # Example usage"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saurav-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
