{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field, SkipValidation\n",
    "from langchain_openai import ChatOpenAI\n",
    "from typing_extensions import TypedDict\n",
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from prompts import GRADE_DOCUMENTS_PROMPT, QUESTION_REWRITER_PROMPT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data model\n",
    "\n",
    "class SharedState(TypedDict):\n",
    "    \"\"\" Shared state for the RAG system. \"\"\"\n",
    "    question: str\n",
    "    agent_response: str\n",
    "    vector_store: Chroma\n",
    "    relevant_documents: list[str]\n",
    "    model: ChatOpenAI\n",
    "\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(\n",
    "        description=\"Documents are relevant to the question, 'yes' or 'no'\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNOWLEDGE_BASE_URLS = [\n",
    "    \"https://www.linkedin.com/pulse/word-embeddings-how-neural-net-understands-words-space-prateek-sbl5c/\",\n",
    "    \"https://www.linkedin.com/pulse/dissecting-backpropagation-neural-networks-saurav-prateek-krcvc/\"\n",
    "]\n",
    "\n",
    "\n",
    "docs = [WebBaseLoader(url).load() for url in KNOWLEDGE_BASE_URLS]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=250, chunk_overlap=0\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "# Add to vectorDB\n",
    "vector_store = Chroma.from_documents(\n",
    "    documents=doc_splits,\n",
    "    collection_name=\"rag-chroma\",\n",
    "    embedding=OpenAIEmbeddings(),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What are vector embeddings?\"\n",
    "documents = vector_store.as_retriever().invoke(question)\n",
    "relevant_docs = [doc.page_content for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "Word embeddings are a fundamental concept in Deep Learning for Natural Language Processing (NLP). They are a way to represent words as dense, real-valued vectors in a multi-dimensional space. The core idea is to capture the semantic and syntactic relationships between words in a way that computers can understand and process.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "In this edition I will explain Word Embeddings through an exercise of analyzing sentiments from a given tweet. In the realm of NLP we send natural language (english sentences) to the neural networks and these networks are not efficient in understanding natural language. They understand numbers and hence we have to convert words into vector embeddings.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Word Embeddings\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Word Embeddings are represented as dense real-valued vectors in a multi-dimensional space. In simple terms, every word is represented as a collection of numerical values which the network can learn.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "How does the neural network learn these embeddings?\n"
     ]
    }
   ],
   "source": [
    "print(len(relevant_docs))\n",
    "print(relevant_docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_documents(shared_state):\n",
    "    \"\"\"\n",
    "    Get relevant documents from the vector store.\n",
    "    \"\"\"\n",
    "    question = shared_state[\"question\"]\n",
    "    vector_store = shared_state[\"vector_store\"]\n",
    "\n",
    "    documents = vector_store.invoke(question)\n",
    "    shared_state[\"relevant_documents\"] = [doc.page_content for doc in documents]\n",
    "\n",
    "    return shared_state\n",
    "\n",
    "\n",
    "def get_model(shared_state):\n",
    "    shared_state['model'] = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "    return shared_state\n",
    "\n",
    "\n",
    "def grade_and_filter_documents(shared_state):\n",
    "    \"\"\"\n",
    "    Grade the relevance of retrieved documents to a user question.\n",
    "    \"\"\"\n",
    "    print(\"\\n\\n Grading documents for relevance... \\n\")\n",
    "    question = shared_state['question']\n",
    "    model = shared_state['model']\n",
    "    documents = shared_state['relevant_documents']\n",
    "    structured_llm_grader = model.with_structured_output(GradeDocuments)\n",
    "\n",
    "    grade_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", GRADE_DOCUMENTS_PROMPT),\n",
    "            (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    retrieval_grader = grade_prompt | structured_llm_grader\n",
    "    filtered_documents = []\n",
    "\n",
    "    for document in documents:\n",
    "        grader_response = retrieval_grader.invoke({\"question\": question, \"document\": document})\n",
    "        if grader_response.binary_score.lower() == \"yes\":\n",
    "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "            filtered_documents.append(document)\n",
    "        else:\n",
    "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "    \n",
    "    print(\"Relevant documents left after filtering:\", len(filtered_documents))\n",
    "    shared_state['relevant_documents'] = filtered_documents\n",
    "\n",
    "    return shared_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer_from_documents(shared_state):\n",
    "    \"\"\" Generate an answer to the question using the relevant documents. \"\"\"\n",
    "    model = shared_state['model']\n",
    "    rag_prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "    question = shared_state['question']\n",
    "    documents = shared_state['relevant_documents']\n",
    "\n",
    "    rag_chain = rag_prompt | model | StrOutputParser()\n",
    "\n",
    "    model_response = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
    "    shared_state['agent_response'] = model_response\n",
    "\n",
    "    return shared_state\n",
    "\n",
    "\n",
    "def decide_to_generate(shared_state):\n",
    "    \"\"\" Decide whether to generate an answer or perform a web search. \"\"\"\n",
    "    if len(shared_state['relevant_documents']) > 0:\n",
    "        print(\"\\n Generating answer from relevant documents... \\n\\n\")\n",
    "        return \"generate\"\n",
    "    else:\n",
    "        print(\"\\n No relevant documents found, transform query and performing web search... \\n\\n\")\n",
    "        return \"transform_query\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_query(shared_state):\n",
    "    \"\"\"\n",
    "    Transform the query to produce a better question.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\n\\n ---TRANSFORMING QUERY---\")\n",
    "    question = shared_state[\"question\"]\n",
    "    model = shared_state[\"model\"]\n",
    "\n",
    "    re_write_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", QUESTION_REWRITER_PROMPT),\n",
    "            (\n",
    "                \"human\",\n",
    "                \"Here is the initial question: \\n\\n {question} \\n Formulate an improved question.\",\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    question_rewriter = re_write_prompt | model | StrOutputParser()\n",
    "    \n",
    "    better_question = question_rewriter.invoke({\"question\": question})\n",
    "    print(\"Transformed question: \\n\", better_question)\n",
    "    shared_state['question'] = better_question\n",
    "    \n",
    "    return shared_state\n",
    "\n",
    "\n",
    "def perform_web_search(shared_state):\n",
    "    \"\"\" Perform a web search to as a fallback. \"\"\"\n",
    "    print(\"\\n\\n Performing a Web Search--- \\n\\n\")\n",
    "\n",
    "    question = shared_state[\"question\"]\n",
    "    web_search_tool = TavilySearchResults(k=3)\n",
    "\n",
    "    docs = web_search_tool.invoke({\"query\": question})\n",
    "    \n",
    "    web_results = [doc[\"content\"] for doc in docs]\n",
    "\n",
    "    shared_state['relevant_documents'] = web_results\n",
    "    return shared_state\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph():\n",
    "    workflow = StateGraph(SharedState)\n",
    "\n",
    "    # Define the nodes\n",
    "    workflow.add_node(\"get_model\", get_model)\n",
    "    workflow.add_node(\"build_vector_store\", build_vector_store)\n",
    "    workflow.add_node(\"get_relevant_documents\", get_relevant_documents)\n",
    "    workflow.add_node(\"grade_and_filter_documents\", grade_and_filter_documents)\n",
    "    workflow.add_node(\"generate_answer_from_documents\", generate_answer_from_documents)\n",
    "    workflow.add_node(\"perform_web_search\", perform_web_search)  # web search\n",
    "    workflow.add_node(\"transform_query\", transform_query)\n",
    "\n",
    "    # Build graph\n",
    "    workflow.add_edge(START, \"get_model\")\n",
    "    workflow.add_edge(\"get_model\", \"build_vector_store\")\n",
    "    workflow.add_edge(\"build_vector_store\", \"get_relevant_documents\")\n",
    "    workflow.add_edge(\"get_relevant_documents\", \"grade_and_filter_documents\")\n",
    "    workflow.add_conditional_edges(\n",
    "        \"grade_and_filter_documents\",\n",
    "        decide_to_generate,\n",
    "        {\n",
    "            \"transform_query\": \"transform_query\",\n",
    "            \"generate\": \"generate_answer_from_documents\",\n",
    "        },\n",
    "    )\n",
    "    workflow.add_edge(\"transform_query\", \"perform_web_search\")\n",
    "    workflow.add_edge(\"perform_web_search\", \"generate_answer_from_documents\")\n",
    "    workflow.add_edge(\"generate_answer_from_documents\", END)\n",
    "\n",
    "    # Compile\n",
    "    return workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Grading documents for relevance... \n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "Relevant documents left after filtering: 0\n",
      "\n",
      " No relevant documents found, transform query and performing web search... \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ---TRANSFORMING QUERY---\n",
      "Transformed question: \n",
      " What is a graph data structure and how is it used in computer science?\n",
      "\n",
      "\n",
      " Performing a Web Search--- \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sauravprateek/Documents/saurav-codes/AI-Engineering-101/saurav-env/lib/python3.13/site-packages/langsmith/client.py:272: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Agent Response \n",
      "\n",
      "A graph data structure is a non-linear structure consisting of vertices (or nodes) connected by edges (or arcs), which can be directed or undirected. In computer science, graphs are used to represent various relationships and processes, such as in social networks, transportation systems, and network design. They are essential for modeling complex systems and analyzing connections between data points.\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "\n",
    "# What is a Graph data-structure?\n",
    "# What are word embeddings and how do they work?\n",
    "compiled_graph = build_graph()\n",
    "shared_state = compiled_graph.invoke({\n",
    "    'question': \"What is a Graph data-structure?\"\n",
    "})\n",
    "\n",
    "print(\"\\n Agent Response \\n\")\n",
    "print(shared_state['agent_response'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saurav-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
