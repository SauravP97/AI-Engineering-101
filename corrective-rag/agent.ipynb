{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field, SkipValidation\n",
    "from langchain_openai import ChatOpenAI\n",
    "from typing_extensions import TypedDict\n",
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from prompts import GRADE_DOCUMENTS_PROMPT, QUESTION_REWRITER_PROMPT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data model\n",
    "\n",
    "class SharedState(TypedDict):\n",
    "    \"\"\" Shared state for the RAG system. \"\"\"\n",
    "    question: str\n",
    "    agent_response: str\n",
    "    vector_store: Chroma\n",
    "    relevant_documents: list[str]\n",
    "    model: ChatOpenAI\n",
    "\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(\n",
    "        description=\"Documents are relevant to the question, 'yes' or 'no'\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNOWLEDGE_BASE_URLS = [\n",
    "    \"https://www.linkedin.com/pulse/word-embeddings-how-neural-net-understands-words-space-prateek-sbl5c/\",\n",
    "    \"https://www.linkedin.com/pulse/dissecting-backpropagation-neural-networks-saurav-prateek-krcvc/\"\n",
    "]\n",
    "\n",
    "\n",
    "def build_vector_store(shared_state):\n",
    "    \"\"\"\n",
    "    Build a vector store from the knowledge base URLs.\n",
    "    \"\"\"\n",
    "    docs = [WebBaseLoader(url).load() for url in KNOWLEDGE_BASE_URLS]\n",
    "    docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "        chunk_size=250, chunk_overlap=0\n",
    "    )\n",
    "    doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "    # Add to vectorDB\n",
    "    vector_store = Chroma.from_documents(\n",
    "        documents=doc_splits,\n",
    "        collection_name=\"rag-chroma\",\n",
    "        embedding=OpenAIEmbeddings(),\n",
    "    )\n",
    "    shared_state['vector_store'] = vector_store.as_retriever()\n",
    "\n",
    "    return shared_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_documents(shared_state):\n",
    "    \"\"\"\n",
    "    Get relevant documents from the vector store.\n",
    "    \"\"\"\n",
    "    question = shared_state[\"question\"]\n",
    "    vector_store = shared_state[\"vector_store\"]\n",
    "\n",
    "    documents = vector_store.invoke(question)\n",
    "    shared_state[\"relevant_documents\"] = [doc.page_content for doc in documents]\n",
    "\n",
    "    return shared_state\n",
    "\n",
    "\n",
    "def get_model(shared_state):\n",
    "    shared_state['model'] = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "    return shared_state\n",
    "\n",
    "\n",
    "def grade_and_filter_documents(shared_state):\n",
    "    \"\"\"\n",
    "    Grade the relevance of retrieved documents to a user question.\n",
    "    \"\"\"\n",
    "    print(\"\\n\\n Grading documents for relevance... \\n\")\n",
    "    question = shared_state['question']\n",
    "    model = shared_state['model']\n",
    "    documents = shared_state['relevant_documents']\n",
    "    structured_llm_grader = model.with_structured_output(GradeDocuments)\n",
    "\n",
    "    grade_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", GRADE_DOCUMENTS_PROMPT),\n",
    "            (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    retrieval_grader = grade_prompt | structured_llm_grader\n",
    "    filtered_documents = []\n",
    "\n",
    "    for document in documents:\n",
    "        grader_response = retrieval_grader.invoke({\"question\": question, \"document\": document})\n",
    "        if grader_response.binary_score.lower() == \"yes\":\n",
    "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "            filtered_documents.append(document)\n",
    "        else:\n",
    "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "    \n",
    "    print(\"Relevant documents left after filtering:\", len(filtered_documents))\n",
    "    shared_state['relevant_documents'] = filtered_documents\n",
    "\n",
    "    return shared_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer_from_documents(shared_state):\n",
    "    \"\"\" Generate an answer to the question using the relevant documents. \"\"\"\n",
    "    model = shared_state['model']\n",
    "    rag_prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "    question = shared_state['question']\n",
    "    documents = shared_state['relevant_documents']\n",
    "\n",
    "    rag_chain = rag_prompt | model | StrOutputParser()\n",
    "\n",
    "    model_response = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
    "    shared_state['agent_response'] = model_response\n",
    "\n",
    "    return shared_state\n",
    "\n",
    "\n",
    "def decide_to_generate(shared_state):\n",
    "    \"\"\" Decide whether to generate an answer or perform a web search. \"\"\"\n",
    "    if len(shared_state['relevant_documents']) > 0:\n",
    "        print(\"\\n Generating answer from relevant documents... \\n\\n\")\n",
    "        return \"generate\"\n",
    "    else:\n",
    "        print(\"\\n No relevant documents found, transform query and performing web search... \\n\\n\")\n",
    "        return \"transform_query\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_query(shared_state):\n",
    "    \"\"\"\n",
    "    Transform the query to produce a better question.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\n\\n ---TRANSFORMING QUERY---\")\n",
    "    question = shared_state[\"question\"]\n",
    "    model = shared_state[\"model\"]\n",
    "\n",
    "    re_write_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", QUESTION_REWRITER_PROMPT),\n",
    "            (\n",
    "                \"human\",\n",
    "                \"Here is the initial question: \\n\\n {question} \\n Formulate an improved question.\",\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    question_rewriter = re_write_prompt | model | StrOutputParser()\n",
    "    \n",
    "    better_question = question_rewriter.invoke({\"question\": question})\n",
    "    print(\"Transformed question: \\n\", better_question)\n",
    "    shared_state['question'] = better_question\n",
    "    \n",
    "    return shared_state\n",
    "\n",
    "\n",
    "def perform_web_search(shared_state):\n",
    "    \"\"\" Perform a web search to as a fallback. \"\"\"\n",
    "    print(\"\\n\\n Performing a Web Search--- \\n\\n\")\n",
    "\n",
    "    question = shared_state[\"question\"]\n",
    "    web_search_tool = TavilySearchResults(k=3)\n",
    "\n",
    "    docs = web_search_tool.invoke({\"query\": question})\n",
    "    \n",
    "    web_results = [doc[\"content\"] for doc in docs]\n",
    "\n",
    "    shared_state['relevant_documents'] = web_results\n",
    "    return shared_state\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph():\n",
    "    workflow = StateGraph(SharedState)\n",
    "\n",
    "    # Define the nodes\n",
    "    workflow.add_node(\"get_model\", get_model)\n",
    "    workflow.add_node(\"build_vector_store\", build_vector_store)\n",
    "    workflow.add_node(\"get_relevant_documents\", get_relevant_documents)\n",
    "    workflow.add_node(\"grade_and_filter_documents\", grade_and_filter_documents)\n",
    "    workflow.add_node(\"generate_answer_from_documents\", generate_answer_from_documents)\n",
    "    workflow.add_node(\"perform_web_search\", perform_web_search)  # web search\n",
    "    workflow.add_node(\"transform_query\", transform_query)\n",
    "\n",
    "    # Build graph\n",
    "    workflow.add_edge(START, \"get_model\")\n",
    "    workflow.add_edge(\"get_model\", \"build_vector_store\")\n",
    "    workflow.add_edge(\"build_vector_store\", \"get_relevant_documents\")\n",
    "    workflow.add_edge(\"get_relevant_documents\", \"grade_and_filter_documents\")\n",
    "    workflow.add_conditional_edges(\n",
    "        \"grade_and_filter_documents\",\n",
    "        decide_to_generate,\n",
    "        {\n",
    "            \"transform_query\": \"transform_query\",\n",
    "            \"generate\": \"generate_answer_from_documents\",\n",
    "        },\n",
    "    )\n",
    "    workflow.add_edge(\"transform_query\", \"perform_web_search\")\n",
    "    workflow.add_edge(\"perform_web_search\", \"generate_answer_from_documents\")\n",
    "    workflow.add_edge(\"generate_answer_from_documents\", END)\n",
    "\n",
    "    # Compile\n",
    "    return workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Grading documents for relevance... \n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "Relevant documents left after filtering: 0\n",
      "\n",
      " No relevant documents found, transform query and performing web search... \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ---TRANSFORMING QUERY---\n",
      "Transformed question: \n",
      " What is a graph data structure and how is it used in computer science?\n",
      "\n",
      "\n",
      " Performing a Web Search--- \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sauravprateek/Documents/saurav-codes/AI-Engineering-101/saurav-env/lib/python3.13/site-packages/langsmith/client.py:272: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Agent Response \n",
      "\n",
      "A graph data structure is a non-linear structure consisting of vertices (or nodes) connected by edges (or arcs), which can be directed or undirected. In computer science, graphs are used to represent various relationships and processes, such as in social networks, transportation systems, and network design. They are essential for modeling complex systems and analyzing connections between data points.\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "\n",
    "# What is a Graph data-structure?\n",
    "# What are word embeddings and how do they work?\n",
    "compiled_graph = build_graph()\n",
    "shared_state = compiled_graph.invoke({\n",
    "    'question': \"What is a Graph data-structure?\"\n",
    "})\n",
    "\n",
    "print(\"\\n Agent Response \\n\")\n",
    "print(shared_state['agent_response'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saurav-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
