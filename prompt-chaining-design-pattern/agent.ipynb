{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SharedState(TypedDict):\n",
    "    query: str\n",
    "    model: ChatOpenAI\n",
    "    from_ml_topic: bool\n",
    "    ai_answer: str\n",
    "\n",
    "class GraderOutput(TypedDict):\n",
    "    from_machine_learning_topic: bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(shared_state: SharedState):\n",
    "    model = ChatOpenAI(model = 'gpt-4o-mini')\n",
    "    shared_state['model'] = model\n",
    "    return shared_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query_topic(shared_state: SharedState):\n",
    "    print(\"Determining if the query is related to machine learning topics...\")\n",
    "    \n",
    "    prompt = \"\"\"\n",
    "You are a classifier that determines if a user's query is related to machine learning topics.\n",
    "Given the user's query, return True if it is related to machine learning topics, otherwise return False.\n",
    "    \"\"\"\n",
    "\n",
    "    model = shared_state['model']\n",
    "    structured_llm_grader = model.with_structured_output(GraderOutput)\n",
    "    query = shared_state['query']\n",
    "\n",
    "    grade_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", prompt),\n",
    "            (\"human\", \"User's Query: \\n\\n {query}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    retrieval_grader = grade_prompt | structured_llm_grader\n",
    "\n",
    "    result = retrieval_grader.invoke({\"query\": query})\n",
    "    shared_state['from_ml_topic'] = result['from_machine_learning_topic']\n",
    "\n",
    "    return shared_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grader_node(shared_state: SharedState):\n",
    "    if shared_state['from_ml_topic']:\n",
    "        return \"continue\"\n",
    "\n",
    "    return \"exit\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_query(shared_state: SharedState):\n",
    "    print(\"Answering the user's query...\")\n",
    "    prompt = \"\"\"\n",
    "You are an expert in machine learning. Answer the user's query under 200 words.\n",
    "    \"\"\"\n",
    "    model = shared_state['model']\n",
    "    query = shared_state['query']\n",
    "    answer_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", prompt),\n",
    "            (\"human\", \"User's Query: \\n\\n {query}\"),\n",
    "        ]\n",
    "    )\n",
    "    answer_chain = answer_prompt | model | StrOutputParser()\n",
    "    result = answer_chain.invoke({\"query\": query})\n",
    "    shared_state['ai_answer'] = result\n",
    "\n",
    "    return shared_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph():\n",
    "    workflow = StateGraph(SharedState)\n",
    "\n",
    "    # Add Nodes\n",
    "    workflow.add_node(build_model, \"build_model\")\n",
    "    workflow.add_node(get_query_topic, \"get_query_topic\")\n",
    "    workflow.add_node(answer_query, \"answer_query\")\n",
    "\n",
    "    workflow.add_edge(START, \"build_model\")\n",
    "    workflow.add_edge(\"build_model\", \"get_query_topic\")\n",
    "    workflow.add_conditional_edges(\n",
    "        \"get_query_topic\", \n",
    "        grader_node, \n",
    "        { \n",
    "            \"continue\": \"answer_query\",\n",
    "            \"exit\": END \n",
    "        }\n",
    "    )\n",
    "    workflow.add_edge(\"answer_query\", END)\n",
    "\n",
    "    return workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query 1: \"What are the latest advancements in machine learning?\"\n",
    "# Query 2: \"What is the capital of India?\"\n",
    "def execute_prompt_chain_workflow():\n",
    "    workflow = build_graph()\n",
    "    initial_state: SharedState = {\n",
    "        \"query\": \"What is the capital of India?\",\n",
    "    }\n",
    "\n",
    "    agent_response = workflow.invoke(initial_state)\n",
    "    print(agent_response)\n",
    "\n",
    "    if agent_response['from_ml_topic']:\n",
    "        print(\"AI's Answer:\", agent_response['ai_answer'])\n",
    "    else:\n",
    "        print(\"The query is not related to machine learning topics.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Determining if the query is related to machine learning topics...\n",
      "{'query': 'What is the capital of India?', 'model': ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x121b37ce0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x1225d11d0>, root_client=<openai.OpenAI object at 0x12231a490>, root_async_client=<openai.AsyncOpenAI object at 0x12231bed0>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')), 'from_ml_topic': False}\n",
      "The query is not related to machine learning topics.\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "execute_prompt_chain_workflow()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saurav-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
