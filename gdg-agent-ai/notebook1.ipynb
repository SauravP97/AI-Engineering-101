{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"gpt-5\"\n",
    "\n",
    "# A Node in LangGraph\n",
    "def node(query):\n",
    "    model = ChatOpenAI(model=MODEL_NAME)\n",
    "    return model.invoke(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Large language models (LLMs) are neural networks trained on vast text to predict next tokens, learning grammar, facts, and patterns. Using transformer architectures, they generate, summarize, translate, and reason over text, and can follow instructions via fine-tuning and reinforcement learning from human feedback. They power chatbots, coding assistants, search, and content tools. Examples include GPT-4, Llama, Claude, and PaLM.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 416, 'prompt_tokens': 16, 'total_tokens': 432, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 320, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-5-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CHYutTbSMA8g8Or5UVwhierlHs56s', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--4d28b9f9-e35a-4f03-8bdd-d080eb236a09-0' usage_metadata={'input_tokens': 16, 'output_tokens': 416, 'total_tokens': 432, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 320}}\n"
     ]
    }
   ],
   "source": [
    "response = node('What are Large Language Models under 100 words?')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large language models (LLMs) are neural networks trained on vast text to predict next tokens, learning grammar, facts, and patterns. Using transformer architectures, they generate, summarize, translate, and reason over text, and can follow instructions via fine-tuning and reinforcement learning from human feedback. They power chatbots, coding assistants, search, and content tools. Examples include GPT-4, Llama, Claude, and PaLM.\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saurav-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
