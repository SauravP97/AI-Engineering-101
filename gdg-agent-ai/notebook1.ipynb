{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"gpt-5\"\n",
    "\n",
    "# A Node in LangGraph\n",
    "def node(query):\n",
    "    model = ChatOpenAI(model=MODEL_NAME)\n",
    "    return model.invoke(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Large Language Models (LLMs) are neural networks trained on vast text to predict the next token. Using the transformer architecture, they learn statistical patterns of language and world knowledge. After pretraining, they can be prompted to generate, summarize, translate, code, and answer questions, often with few-shot examples. They rely on context windows, not memory, and may produce plausible but false outputs, reflect bias, or be sensitive to prompts. Fine-tuning and tools can specialize or extend them.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 492, 'prompt_tokens': 16, 'total_tokens': 508, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 384, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-5-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CHlpNKZq5eXQTq0nRux9bRlBHssmZ', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--ec564e8d-6ad5-41ce-a1e1-659f99b8f720-0' usage_metadata={'input_tokens': 16, 'output_tokens': 492, 'total_tokens': 508, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 384}}\n"
     ]
    }
   ],
   "source": [
    "response = node('What are Large Language Models under 100 words?')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large Language Models (LLMs) are neural networks trained on vast text to predict the next token. Using the transformer architecture, they learn statistical patterns of language and world knowledge. After pretraining, they can be prompted to generate, summarize, translate, code, and answer questions, often with few-shot examples. They rely on context windows, not memory, and may produce plausible but false outputs, reflect bias, or be sensitive to prompts. Fine-tuning and tools can specialize or extend them.\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saurav-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
